---
title: Is Anything Worth Maximizing?
date: 2018-09-06 20:20:18 +0000
updated: 2019-11-15 00:02:47 +0000
source: https://medium.com/what-to-build/is-anything-worth-maximizing-d11e648eb56f
tags:
  - goals #goals
  - metrics #metrics
  - value #value
---
Metrics are at the heart of how algorithms — like YouTube Recommended — and organizations — like YouTube itself — work... If you think of an organization as a team, the metric is the team’s “score” – the number that everyone is working to increase. The number decides:-   how to market,
-   which features to build,
-   which countries to launch in,
-   who to hire,
-   everything the organization decides to do

The master metric at YouTube is *the total time people spent viewing videos*, adding up all of YouTube’s users. So YouTube makes decisions that increase the total time people spend watching videos.
__Once metrics are defined,__ __they’re like parasites, or undead spirits. They take over human beings.__ I mean, nobody who works for YouTube *really* wants to increase the time people spend watching videos, but together that’s what all those people do.
/
__Knowing Behavior vs. Knowing Reasons__
What do all these metrics have in common? They all focus on the behavior of users. I believe this is a problem. __By focusing on the behavior of users we’re missing something important about them, and we’re serving them poorly.__
What is it to understand someone as a person? Is it more about knowing their behavior — when they get up in the morning, when they sleep, how much water they drink, what kinds of thumbnails they click on? Or is it more about knowing their reasons — like that they want to learn an instrument, or face their fears, or that they want to be more creative, or that they are deeply committed to their child?
![](Is%20Anything%20Worth%20Maximizing_.html.resources/0822AA47-B33E-42CF-A9C6-93149C5262B7.png)
/
__Choosing for Reasons vs. Choosing for Outcomes____Responsibility and Growth__
For simple maximizers, its choices are just about numbers. That means its choices are *in* the numbers. Here, the choice between two desserts is just a choice between numbers. We could say its choice is *already made*. And that __it has no__ __*responsibility*____, since it’s just following what the numbers say__.
Reason-based maximizers don’t just see numbers, though, they also see values. Here, there’s a choice between two desserts — but it isn’t a choice between two numbers. See, it’s also a choice between two *values*. One option means being a seize-the-day, *intensity* kind of person. The other means being a foody, aristocratic, *elegance* kind of person.
/
__Trust__
Simple maximizers tend to betray one another. That’s what the prisoner’s dilemma is about.Deep trust requires a conversation, where we discover *shared reasons*. One agent has reasons, the other agent sees them. The second agent develops reasons that fit together with the first agent’s reasons , and the first agent sees how the reasons fit together, and that creates the trust.
__Why do people trust each other? Because the__ __*know*__ __their own reasons, because their reasons__ __*fit together*__ __with another person’s, and because the reasons involved are__ __*durable*__ __— they won’t change during the process of cooperation.__
While reputation systems and guns and sensitivity training might get shady people to be slightly less shady, they don’t create real trust.
If this is what trust is, it can’t happen with simple maximizers. They have metrics, but they don’t have reasons or an identity, so they can’t possibly share reasons with us.__If most organizations are simple maximizers, that means that organizations don’t deserve our trust.__
But \[small businesses\] have a limit to the number of people they can serve. Government bureaucracies, large businesses, and nonprofits have no hope of listening to everyone individually, and they need to use some kinds of metrics and analytics in order to be a team. Big organizations and even algorithms could become reason-based maximizers, if they adopt a certain special kind of metric.
/
One thing that can happen, is I stop trusting YouTube. I see it doesn’t share my reasons. Maybe I give up on consumer technology, switch to a feature phone, or go to Camp Grounded.That’s the better possibility, I think.
The other thing that can happen, is that I *do* sort of trust YouTube. And YouTube uses that trust to find a way to manipulate me.
How would it do that? Well, I mentioned YouTube’s metrics are about my behavior. YouTube wants my behavior to go a certain way, regardless of my reasons. So YouTube is on the hunt for ways to steer my behavior. To discover what converts me to the behaviors they like.
__In the__ __*worst*__ __case, YouTube acts to stimulate the behaviors it wants, every time I come. I forget why I came. I forget my reasons — my identity. YouTube might even give me a new identity, one that drives its metrics.__ So on Twitter, I’d start focusing on maximizing my follower count or my likes, and on YouTube, I’d focus on spending time laughing alone.
This isn’t cooperation. Instead, I’ve become part of the machine. I’ve become a simple maximizer myself.
And maybe no one at YouTube even intended this! __When simple maximizing metrics guide product decisions and algorithms, manipulative features will be successful even if they’re accidental.__ The entire tech ecosystem can become glutted with features that are *inadvertently manipulative*.
/
__Flight to Higher Ground__
The old metric — which snake oil did well on — was *sales.* The new one: *medically justifiable sales*. This triggered a flight to higher ground. Consumers trusted the new drug assessments, from this nonprofit, because its metric was aligned with their real reasons. They stopped buying drugs from the snake oil guys. The entire market was restructured around the new metric, *medically justifiable sales*.
Harvey Wiley’s nonprofit became part of the government, *the Food and Drug Administration*.But Wiley’s metric wasn’t perfect. Health companies found a new way to exploit consumers — by selling super expensive procedures and medicines, instead of the cheap ones.
Fortunately, this guy Sidney Garfield started a new flight to higher ground in health. The old metric was *medically justifiable sales*, the new one was *community health levels*. See, Sidney Garfield formed a network of hospitals, and he paid those hospitals by how well they kept their whole area healthy. His network — Kaiser Permanente — was more trustworthy and less conflicted for consumers, so people flocked to it. This way of paying hospitals spread to other countries. It restructured the health sector globally, and it’s found a slow way forward in the US too.